{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Raw Processing - Health Insurance Market Data**\n",
        "\n",
        "*   **Turma:** eEDB-015/2025-1 - Projeto Integrador\n",
        "*   **Grupo:** H"
      ],
      "metadata": {
        "id": "nf3zh9W342Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "fj-dv1CJ49m0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGE5Eg-n40qq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import hashlib\n",
        "import traceback\n",
        "import gc\n",
        "from typing import Dict, List, Optional, Union\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import json\n",
        "from jsonschema import validate\n",
        "from datetime import datetime\n",
        "import boto3\n",
        "import logging\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constantes"
      ],
      "metadata": {
        "id": "AZpAizzD5EFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do logger\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Configuração do cliente S3\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# Constantes\n",
        "INPUT_BUCKET = 'your-input-bucket-name' # ALTERAR!\n",
        "OUTPUT_BUCKET = 'your-output-bucket-name' # ALTERAR!\n",
        "ZIP_FILE_KEY = 'path/to/your/zipfile.zip' # ALTERAR!\n",
        "\n",
        "# Anos para processar\n",
        "years_to_process = ['2014', '2015', '2016']"
      ],
      "metadata": {
        "id": "hahAnoMj5Fqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções Auxiliares"
      ],
      "metadata": {
        "id": "Lwgnlgp85HZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_file_name(file_name: str) -> str:\n",
        "    return re.sub(r\"_PUF.*\", \"\", file_name)\n",
        "\n",
        "def ensure_s3_directory(bucket: str, path: str) -> None:\n",
        "    try:\n",
        "        s3.put_object(Bucket=bucket, Key=(path.rstrip('/') + '/'))\n",
        "        logger.info(f\"Diretório S3 criado: s3://{bucket}/{path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro ao criar diretório S3: {str(e)}\")\n",
        "\n",
        "def read_csv_from_s3(bucket: str, key: str, chunksize: Optional[int] = None) -> Union[pd.DataFrame, pd.io.parsers.TextFileReader]:\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        return pd.read_csv(BytesIO(obj['Body'].read()), chunksize=chunksize)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro ao ler CSV do S3: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def validate_data(df: pd.DataFrame) -> Dict:\n",
        "    total_rows = len(df)\n",
        "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "    other_columns = df.columns.difference(numeric_columns)\n",
        "    numeric_null_counts = df[numeric_columns].isnull().sum() if not numeric_columns.empty else pd.Series(dtype=float)\n",
        "    other_null_counts = df[other_columns].isnull().sum() if not other_columns.empty else pd.Series(dtype=float)\n",
        "    null_counts = pd.concat([numeric_null_counts, other_null_counts])\n",
        "\n",
        "    validation_results = {\n",
        "        \"total_rows\": total_rows,\n",
        "        \"columns\": df.columns.tolist(),\n",
        "        \"null_percentages\": (null_counts / total_rows * 100).to_dict()\n",
        "    }\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "def is_csv_file(filename: str) -> bool:\n",
        "    return filename.lower().endswith('.csv')\n",
        "\n",
        "def generate_version_hash(df: pd.DataFrame) -> str:\n",
        "    return hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()"
      ],
      "metadata": {
        "id": "WkfCUAp15HIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções Auxiliares - Específicas para Camada **RAW**"
      ],
      "metadata": {
        "id": "hxGNJVs25Wh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_save_file(bucket: str, key: str, table_name: str) -> None:\n",
        "    try:\n",
        "        logger.info(f\"Começando o processamento do arquivo: {key}\")\n",
        "        if key.startswith('.') or not is_csv_file(key):\n",
        "            logger.warning(f\"Ignorando arquivo não CSV ou oculto: {key}\")\n",
        "            return\n",
        "\n",
        "        # Normalizar o nome da tabela\n",
        "        table_name = normalize_file_name(os.path.basename(key))\n",
        "\n",
        "        # Leitura em chunks para otimização de memória\n",
        "        chunk_size = 100000  # Ajuste conforme necessário\n",
        "        chunks = read_csv_from_s3(bucket, key, chunksize=chunk_size)\n",
        "\n",
        "        table_path = f\"{table_name}/\"\n",
        "        ensure_s3_directory(OUTPUT_BUCKET, table_path)\n",
        "\n",
        "        # Colunas para comparação (excluindo ingestDate, partitionDate e version)\n",
        "        compare_columns = None\n",
        "        new_records_total = 0\n",
        "        processed_chunks = 0\n",
        "\n",
        "        for chunk in chunks:\n",
        "            processed_chunks += 1\n",
        "            logger.info(f\"Processando chunk {processed_chunks} para {table_name}\")\n",
        "\n",
        "            if chunk.empty:\n",
        "                continue\n",
        "\n",
        "            chunk = chunk.infer_objects()\n",
        "            chunk['ingestDate'] = pd.Timestamp.now()\n",
        "            chunk['partitionDate'] = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
        "            chunk['version'] = generate_version_hash(chunk)\n",
        "\n",
        "            if compare_columns is None:\n",
        "                compare_columns = [col for col in chunk.columns if col not in [\"ingestDate\", \"partitionDate\", \"version\"]]\n",
        "\n",
        "            # Ler dados existentes (se houver)\n",
        "            existing_df = pd.DataFrame()\n",
        "            try:\n",
        "                existing_objects = s3.list_objects_v2(Bucket=OUTPUT_BUCKET, Prefix=table_path)\n",
        "                if 'Contents' in existing_objects:\n",
        "                    existing_df = pd.concat([\n",
        "                        pd.read_parquet(BytesIO(s3.get_object(Bucket=OUTPUT_BUCKET, Key=obj['Key'])['Body'].read()))\n",
        "                        for obj in existing_objects['Contents']\n",
        "                        if obj['Key'].endswith('.parquet')\n",
        "                    ])\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao ler dados existentes para {table_name}: {str(e)}\")\n",
        "\n",
        "            if not existing_df.empty:\n",
        "                # Identificar registros únicos em chunk\n",
        "                merged = chunk.merge(existing_df[compare_columns + ['partitionDate']],\n",
        "                                      on=compare_columns,\n",
        "                                      how='left',\n",
        "                                      suffixes=('', '_existing'))\n",
        "\n",
        "                # Manter apenas registros novos ou atualizações em partições mais recentes\n",
        "                new_records = merged[\n",
        "                    (merged['partitionDate_existing'].isnull()) |\n",
        "                    (merged['partitionDate'] > merged['partitionDate_existing'])\n",
        "                ][chunk.columns]\n",
        "\n",
        "                # Combinar registros existentes com novos registros únicos\n",
        "                combined_df = pd.concat([existing_df, new_records], ignore_index=True)\n",
        "            else:\n",
        "                combined_df = chunk\n",
        "                new_records = chunk\n",
        "\n",
        "            # Remover duplicatas, mantendo o registro mais antigo\n",
        "            final_df = combined_df.sort_values('ingestDate').drop_duplicates(subset=compare_columns, keep='first')\n",
        "\n",
        "            new_records_total += len(new_records)\n",
        "\n",
        "            # Validar os dados\n",
        "            validation_results = validate_data(final_df)\n",
        "            logger.info(f\"Validação para {table_name}: {validation_results}\")\n",
        "\n",
        "            # Salvar os dados no S3\n",
        "            buffer = BytesIO()\n",
        "            final_df.to_parquet(buffer, engine='pyarrow', compression='snappy', index=False)\n",
        "            buffer.seek(0)\n",
        "            s3.put_object(Bucket=OUTPUT_BUCKET, Key=f\"{table_path}{table_name}_{processed_chunks}.parquet\", Body=buffer.getvalue())\n",
        "            logger.info(f\"Chunk {processed_chunks} processado e salvo para a tabela {table_name}\")\n",
        "\n",
        "            # Liberar memória\n",
        "            del chunk, combined_df, final_df, existing_df\n",
        "            if 'merged' in locals():\n",
        "                del merged\n",
        "            if 'new_records' in locals():\n",
        "                del new_records\n",
        "            gc.collect()\n",
        "\n",
        "        logger.info(f\"Processamento concluído para {key}\")\n",
        "        logger.info(f\"Total de novos registros para {table_name}: {new_records_total}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro ao processar {key}: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())"
      ],
      "metadata": {
        "id": "9sx0LKVJ5XN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_zip_file():\n",
        "    try:\n",
        "        zip_obj = s3.get_object(Bucket=INPUT_BUCKET, Key=ZIP_FILE_KEY)\n",
        "        zip_content = BytesIO(zip_obj['Body'].read())\n",
        "\n",
        "        # Processar o conteúdo do zip aqui\n",
        "        # Por exemplo, você pode usar a biblioteca zipfile para extrair os arquivos\n",
        "\n",
        "        # Após extrair, processe cada arquivo CSV\n",
        "        for file_name in extracted_files:\n",
        "            if is_csv_file(file_name):\n",
        "                table_name = f\"tb_{normalize_file_name(file_name).lower()}\"\n",
        "                process_and_save_file(INPUT_BUCKET, file_name, table_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro ao processar o arquivo ZIP: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())"
      ],
      "metadata": {
        "id": "2AdUtQwD5cXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "lKXydfte5hLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    process_zip_file()"
      ],
      "metadata": {
        "id": "yjt7PXPe5kcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}